{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Kaggle Assignment\n",
    "\n",
    "## Info\n",
    "* This assignment is solving a data challenge on Kaggle. \n",
    "* You will graded based upon the score you get on Kaggle. \n",
    "\n",
    "## Setup\n",
    "\n",
    "* Download [Anaconda Python 3.6](https://www.anaconda.com/download/) for consistent environment.\n",
    "* If you use pip environment then make sure your code is compatible with versions of libraries provided withing Anaconda's Python 3.6 distribution.\n",
    "\n",
    "## Submission\n",
    "* Make sure you submit all your code in ZIP file on the learn, it must contain a TEXT file (.txt) containing explanation for your approach (just paragraph or two nicely explained in bullet points).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Notes\n",
    "(Please write any notes here that you think I should know during marking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stems words to their root words and removes all characters that are not alphabets\n",
    "def stem_str(str):\n",
    "    ret_str = \"\"\n",
    "    for w in word_tokenize(str.lower()):\n",
    "        if w not in stop_words and w.isalpha():\n",
    "            ret_str = ret_str + \" \" + ps.stem(w)\n",
    "    return ret_str.strip()\n",
    "\n",
    "# Gets the count of most frequent words give a dataframe\n",
    "def word_freq(df):\n",
    "    word_frequency = {}\n",
    "    for index,row in df.iterrows():\n",
    "        for w in word_tokenize(row['stemmed_sms']):\n",
    "            if w not in word_frequency:\n",
    "                word_frequency[w] = 1\n",
    "            else:\n",
    "                word_frequency[w] += 1\n",
    "    return word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     category                                               text\n",
      "0         ham  Go until jurong point, crazy.. Available only ...\n",
      "1         ham                      Ok lar... Joking wif u oni...\n",
      "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3         ham  U dun say so early hor... U c already then say...\n",
      "4         ham  Nah I don't think he goes to usf, he lives aro...\n",
      "5        spam  FreeMsg Hey there darling it's been 3 week's n...\n",
      "6         ham  Even my brother is not like to speak with me. ...\n",
      "7         ham  As per your request 'Melle Melle (Oru Minnamin...\n",
      "8        spam  WINNER!! As a valued network customer you have...\n",
      "9        spam  Had your mobile 11 months or more? U R entitle...\n",
      "10        ham  I'm gonna be home soon and i don't want to tal...\n",
      "11       spam  SIX chances to win CASH! From 100 to 20,000 po...\n",
      "12       spam  URGENT! You have won a 1 week FREE membership ...\n",
      "13        ham  I've been searching for the right words to tha...\n",
      "14        ham                I HAVE A DATE ON SUNDAY WITH WILL!!\n",
      "15       spam  XXXMobileMovieClub: To use your credit, click ...\n",
      "16        ham                         Oh k...i'm watching here:)\n",
      "17        ham  Eh u remember how 2 spell his name... Yes i di...\n",
      "18        ham  Fine if thatåÕs the way u feel. ThatåÕs the wa...\n",
      "19       spam  England v Macedonia - dont miss the goals/team...\n",
      "20        ham          Is that seriously how you spell his name?\n",
      "21        ham  IÛ÷m going to try for 2 months ha ha only joking\n",
      "22        ham  So Ì_ pay first lar... Then when is da stock c...\n",
      "23        ham  Aft i finish my lunch then i go str down lor. ...\n",
      "24        ham  Ffffffffff. Alright no way I can meet up with ...\n",
      "25        ham  Just forced myself to eat a slice. I'm really ...\n",
      "26        ham                     Lol your always so convincing.\n",
      "27        ham  Did you catch the bus ? Are you frying an egg ...\n",
      "28        ham  I'm back &amp; we're packing the car now, I'll...\n",
      "29        ham  Ahhh. Work. I vaguely remember that! What does...\n",
      "...       ...                                                ...\n",
      "5542      ham           Armand says get your ass over to epsilon\n",
      "5543      ham             U still havent got urself a jacket ah?\n",
      "5544      ham  I'm taking derek &amp; taylor to walmart, if I...\n",
      "5545      ham      Hi its in durban are you still on this number\n",
      "5546      ham         Ic. There are a lotta childporn cars then.\n",
      "5547     spam  Had your contract mobile 11 Mnths? Latest Moto...\n",
      "5548      ham                 No, I was trying it all weekend ;V\n",
      "5549      ham  You know, wot people wear. T shirts, jumpers, ...\n",
      "5550      ham        Cool, what time you think you can get here?\n",
      "5551      ham  Wen did you get so spiritual and deep. That's ...\n",
      "5552      ham  Have a safe trip to Nigeria. Wish you happines...\n",
      "5553      ham                        Hahaha..use your brain dear\n",
      "5554      ham  Well keep in mind I've only got enough gas for...\n",
      "5555      ham  Yeh. Indians was nice. Tho it did kane me off ...\n",
      "5556      ham  Yes i have. So that's why u texted. Pshew...mi...\n",
      "5557      ham  No. I meant the calculation is the same. That ...\n",
      "5558      ham                             Sorry, I'll call later\n",
      "5559      ham  if you aren't here in the next &lt;#&gt; hours...\n",
      "5560      ham                  Anything lor. Juz both of us lor.\n",
      "5561      ham  Get me out of this dump heap. My mom decided t...\n",
      "5562      ham  Ok lor... Sony ericsson salesman... I ask shuh...\n",
      "5563      ham                                Ard 6 like dat lor.\n",
      "5564      ham  Why don't you wait 'til at least wednesday to ...\n",
      "5565      ham                                       Huh y lei...\n",
      "5566     spam  REMINDER FROM O2: To get 2.50 pounds free call...\n",
      "5567     spam  This is the 2nd time we have tried 2 contact u...\n",
      "5568      ham              Will Ì_ b going to esplanade fr home?\n",
      "5569      ham  Pity, * was in mood for that. So...any other s...\n",
      "5570      ham  The guy did some bitching but I acted like i'd...\n",
      "5571      ham                         Rofl. Its true to its name\n",
      "\n",
      "[5572 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#reading in the data and renaming columns    \n",
    "data = pd.read_csv('./spam.csv',encoding = \"ISO-8859-1\")\n",
    "data.columns = ['category', 'text']\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace ham and spam with 0 and 1\n",
    "data['category'] = data['category'].replace(['ham','spam'],[0,1])\n",
    "\n",
    "y = data['category'].as_matrix()\n",
    "X_text = data['text'].as_matrix() \n",
    "data['stemmed_sms'] = data.loc[:,'text'].apply(lambda x: stem_str(str(x)))\n",
    "X_text_stem = data['stemmed_sms'].as_matrix() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#CountVectorizer alone better accuracy than with TfidfVectorizer\n",
    "sw = stopwords.words(\"english\")\n",
    "cv = CountVectorizer(stop_words =sw)\n",
    "X_stem = cv.fit_transform(X_text).toarray()\n",
    "\n",
    "X = cv.fit_transform(X_text_stem).toarray()\n",
    "\n",
    "print(X_stem)\n",
    "print(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini\n",
      "0.964114832536\n",
      "0.905829596413\n",
      "entropy\n",
      "0.960526315789\n",
      "0.918660287081\n"
     ]
    }
   ],
   "source": [
    "#train tree\n",
    "clf_gini = tree.DecisionTreeClassifier(criterion=\"gini\")\n",
    "clf_entropy = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "#gini\n",
    "print(\"Gini\")\n",
    "clf_gini.fit(X_train,y_train)\n",
    "predtree = clf_gini.predict(X_test)\n",
    "\n",
    "#print('accuracy:')\n",
    "print(accuracy_score(y_test,predtree))\n",
    "\n",
    "#print('precision:')\n",
    "print(precision_score(y_test,predtree))\n",
    "\n",
    "print(\"entropy\")\n",
    "clf_entropy.fit(X_train,y_train)\n",
    "predtree = clf_entropy.predict(X_test)\n",
    "print(accuracy_score(y_test,predtree))\n",
    "print(precision_score(y_test,predtree))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini\n",
      "[[100, 0.96306999999999998, 0.85955499999999996]]\n",
      "entropy\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "cv_acc_scores = []\n",
    "cv_prec_scores = []\n",
    "print(\"Gini\")\n",
    "# perform 10-fold cross validation\n",
    "acc_scores = cross_val_score(clf_gini, X_train, y_train, cv=10, scoring='accuracy')\n",
    "prec_scores = cross_val_score(clf_gini, X_train, y_train, cv=10, scoring='precision')\n",
    "\n",
    "scores.append([100, round(acc_scores.mean(), 6), round(prec_scores.mean(), 6)])\n",
    "print(scores)\n",
    "\n",
    "scores = []\n",
    "\n",
    "print(\"entropy\")\n",
    "acc_scores = cross_val_score(clf_entropy, X_train, y_train, cv=10, scoring='accuracy')\n",
    "prec_scores = cross_val_score(clf_entropy, X_train, y_train, cv=10, scoring='precision')\n",
    "\n",
    "scores.append([100, round(acc_scores.mean(), 6), round(prec_scores.mean(), 6)])\n",
    "print(scores)\n"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/aff9e6e8ebaf9e733ae5b58405965b6b"
  },
  "gist": {
   "data": {
    "description": "projects/SYDE-522/Tutorials/Assignments/Assignment 1/Tutorial Assignment 1.ipynb",
    "public": false
   },
   "id": "aff9e6e8ebaf9e733ae5b58405965b6b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
